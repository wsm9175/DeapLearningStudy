{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LossFunction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEDcAD6x9vPmMYN9ONo+Fw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsm9175/DeepLearningStudy/blob/main/LossFunction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-1-1: Dataset for Regression"
      ],
      "metadata": {
        "id": "9K6zeRMPaogA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUF2NJ1NYk5R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "N, n_feature = 8, 5\n",
        "#constant - 상수 tensor 생성, dtype=tf.float32 중요\n",
        "t_weights = tf.constant([1, 2, 3, 4, 5], dtype = tf.float32) # 목표 weights\n",
        "t_bias = tf.constant([10], dtype=tf.float32) # 목표 bias\n",
        "# mean -  평균,  stddev - 표준편차\n",
        "X = tf.random.normal(mean = 0, stddev=1, shape=(N, n_feature))\n",
        "Z = tf.reduce_sum(t_weights*X, axis=1) + t_bias"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code. 4-1-2: Dataset for Binary Classification"
      ],
      "metadata": {
        "id": "LS34gN7tdo8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "N, n_feature = 8, 5\n",
        "t_weights = tf.constant([1, 2, 3, 4, 5], dtype = tf.float32)\n",
        "t_bias = tf.constant([10], dtype=tf.float32)\n",
        "\n",
        "X = tf.random.normal(mean=0, stddev=1, shape=(N, n_feature))\n",
        "Z = tf.reduce_sum(t_weights*X, axis=1) + t_bias\n",
        "Z = tf.cast(Z > 5, tf.int32)"
      ],
      "metadata": {
        "id": "XpuoSI--bhWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code.4-1-3:Dataset for Multi-class Classification"
      ],
      "metadata": {
        "id": "MjWucVXtfVOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "N, n_feature = 30, 2\n",
        "N_class = 5\n",
        "\n",
        "# 데이터를 담을 껍데기 생성\n",
        "X = tf.zeros(shape=(0, n_feature))\n",
        "Y = tf.zeros(shape=(0, 1), dtype=tf.int32)\n",
        "\n",
        "for class_idx in range(N_class):\n",
        "  # tf.random.uniform - 균등 분포\n",
        "  center = tf.random.uniform(minval=-15, maxval=15, shape=(2, 1))\n",
        "  x1 = center[0] + tf.random.normal(shape=(N, 1))\n",
        "  x2 = center[1] + tf.random.normal(shape=(N, 1))\n",
        "\n",
        "  x = tf.concat((x1, x2), axis=1) # (30, 2)\n",
        "  # 각 데이터가 속한 클래스 index 설정\n",
        "  y = class_idx*tf.ones(shape=(N, 1), dtype=tf.int32) \n",
        "   \n",
        "  X = tf.concat((X, x), axis=0) # (30, 2)\n",
        "  Y = tf.concat((Y, y), axis=0) # (30, 1)\n",
        "\n",
        "  # print(center.shape)\n",
        "  # print(x1.shape)\n",
        "  # print(x2.shape)\n",
        "  # print(x.shape)\n",
        "  # print(y.shape)\n",
        "  # print(X.shape)\n",
        "  # print(Y.shape)\n",
        "  # print(X, Y)\n",
        "  # print('==============')\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "rzxNHb0-edVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-1-4: Dataset for Multi-class Classification with One-hot Encoding"
      ],
      "metadata": {
        "id": "GxFNZJY5HHps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "N, n_feature = 30, 2\n",
        "N_class = 5\n",
        "\n",
        "X = tf.zeros(shape=(0, n_feature))\n",
        "Y = tf.zeros(shape=(0,), dtype=tf.int32)\n",
        "\n",
        "for class_idx in range(N_class):\n",
        "  center = tf.random.uniform(minval=-15, maxval = 15, shape=(2, 1))\n",
        "\n",
        "  x1 = center[0] + tf.random.normal(shape = (N, 1))\n",
        "  x2 = center[1] + tf.random.normal(shape = (N, 1))\n",
        "\n",
        "  x = tf.concat((x1, x2), axis=1)\n",
        "  y = class_idx*tf.ones(shape=(N, ), dtype=tf.int32)\n",
        "\n",
        "  X = tf.concat((X, x), axis=0)\n",
        "  Y = tf.concat((Y, y), axis=0)\n",
        "\n",
        "  print(X.shape, Y.shape)\n",
        "Y = tf.one_hot(Y, depth=N_class, dtype=tf.int32)\n",
        "print(Y.numpy())"
      ],
      "metadata": {
        "id": "oE0RAeTmgALQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-1-5: Dataset Object"
      ],
      "metadata": {
        "id": "-yis3f9vihSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "N, n_feature = 100, 5\n",
        "batch_size = 32\n",
        "\n",
        "t_weights = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n",
        "t_bias = tf.constant([10], dtype=tf.float32)\n",
        "\n",
        "X = tf.random.normal(mean=0, stddev=1, shape=(N, n_feature))\n",
        "Y = tf.reduce_sum(t_weights*X, axis=1) + t_bias\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "for x, y in dataset:\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH8hoDuTHk47",
        "outputId": "1fc07c0b-e422-4434-c5e9-3eb3922dad5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 5) (32,)\n",
            "(32, 5) (32,)\n",
            "(32, 5) (32,)\n",
            "(4, 5) (4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-2: Mean Squared Error"
      ],
      "metadata": {
        "id": "0kB1DhV3mUPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-2-1: MSE Calculation"
      ],
      "metadata": {
        "id": "81BEYmilmakx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "loss_object = MeanSquaredError() # 실제값과 예측값을 비교해줄 object\n",
        "\n",
        "batch_size = 32\n",
        "prediction = tf.random.normal(shape=(batch_size, 1))\n",
        "labels = tf.random.normal(shape=(batch_size, 1))\n",
        "\n",
        "mse = loss_object(labels, prediction)\n",
        "mse_manual = tf.reduce_mean(tf.math.pow(labels - prediction, 2))\n",
        "\n",
        "print(\"MSE(TF): \", mse.numpy())\n",
        "print(\"MSE(manual): \", mse_manual.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkb1HCdbjzA0",
        "outputId": "ad1ca8bc-8065-4202-eb16-ef840715ba8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE(TF):  1.9037309\n",
            "MSE(manual):  1.9037309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-2-2: MSE with Model/Dataset"
      ],
      "metadata": {
        "id": "1RDrYIQGsjFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "N, n_feature = 100, 5\n",
        "batch_size = 32\n",
        "\n",
        "X = tf.random.normal(shape=(N, n_feature))\n",
        "Y = tf.random.normal(shape=(N, 1))\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "dense = Dense(units=1, activation='linear')\n",
        "loss_object = MeanSquaredError()\n",
        "\n",
        "for x, y in dataset:\n",
        "  predictions = dense(x)\n",
        "  loss = loss_object(y, predictions)\n",
        "  print(loss.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUMeOcrzn__e",
        "outputId": "f2cfd4b1-48e4-467b-9fc1-e5c0ad8cc1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.498843\n",
            "2.3197222\n",
            "1.2070175\n",
            "2.384316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-3: binary Cross Entropy"
      ],
      "metadata": {
        "id": "v-mdJcbAvTjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-3-1: BCE Calculation"
      ],
      "metadata": {
        "id": "RDLlYnmivbSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "batch_size = 2\n",
        "n_class = 2\n",
        "\n",
        "predictions = tf.random.uniform(shape=(batch_size, 1),\n",
        "                                minval=0, maxval=1,\n",
        "                                dtype=tf.float32)\n",
        "\n",
        "labels = tf.random.uniform(shape=(batch_size, 1), \n",
        "                           minval=0, maxval=n_class,\n",
        "                           dtype=tf.int32)\n",
        "\n",
        "loss_object = BinaryCrossentropy()\n",
        "loss = loss_object(labels, predictions)\n",
        "\n",
        "labels = tf.cast(labels, tf.float32)\n",
        "bce_man = -(labels*tf.math.log(predictions) + (1-labels)*tf.math.log(1-predictions))\n",
        "bce_man = tf.reduce_mean(bce_man)\n",
        "\n",
        "print(\"MSE(TF): \", loss.numpy())\n",
        "print(\"MSE(MAN): \", bce_man.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBns7P0Ct7ki",
        "outputId": "24fbc248-d996-4225-f1d5-b72df282b4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE(TF):  0.30258635\n",
            "MSE(MAN):  0.30258653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-3-2: BCE with model/Dataset"
      ],
      "metadata": {
        "id": "l-e1JDGbzaIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "N, n_feature = 100, 5\n",
        "t_weights = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n",
        "t_bias = tf.constant([10], dtype=tf.float32)\n",
        "\n",
        "X = tf.random.normal(mean=0, stddev=1, shape=(N, n_feature))\n",
        "Y = tf.reduce_sum(t_weights*X, axis=1) + t_bias \n",
        "Y = tf.cast(Y>5, tf.int32)\n",
        "print(X.shape, Y.shape)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "model = Dense(units=1, activation='sigmoid')\n",
        "loss_object = BinaryCrossentropy()\n",
        "\n",
        "for x, y in dataset:\n",
        "  predictions = model(x)\n",
        "  loss = loss_object(y, predictions)\n",
        "  print(loss.numpy())"
      ],
      "metadata": {
        "id": "inxebeRfvl9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-4:Sparse Categorical Cross Entropy"
      ],
      "metadata": {
        "id": "rGr35KGo1ZkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-4-1:SCCE calculation"
      ],
      "metadata": {
        "id": "tMPWqIHc1ewD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "batch_size, n_class = 16, 5\n",
        "\n",
        "predictions = tf.random.uniform(shape=(batch_size, n_class), \n",
        "                                minval =  0, maxval = 1,\n",
        "                                dtype = tf.float32)\n",
        "pred_sum = tf.reshape(tf.reduce_sum(predictions, axis=1), (-1, 1)) # raw합\n",
        "\n",
        "predictions = predictions/pred_sum\n",
        "\n",
        "labels = tf.random.uniform(shape=(batch_size, ),\n",
        "                           minval=0, maxval=n_class,\n",
        "                           dtype=tf.int32)\n",
        "\n",
        "loss_object = SparseCategoricalCrossentropy()\n",
        "loss = loss_object(labels, predictions)\n",
        "\n",
        "#manual\n",
        "ce=0\n",
        "for label, prediction in zip(labels, predictions):\n",
        "  ce += -tf.math.log(prediction[label])\n",
        "ce /= batch_size\n",
        "print(loss.numpy())\n",
        "print(ce.numpy()) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYkyAZYizxLs",
        "outputId": "4a4cc6f4-b6d1-4f57-ac7f-3edd9ce6c1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6114346\n",
            "1.6114345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cde.4-4-2: SCCE with Model/Dataset"
      ],
      "metadata": {
        "id": "9pTDZu8xm8Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "N, n_feature = 100, 2\n",
        "n_class = 5\n",
        "batch_size = 32\n",
        "\n",
        "X = tf.zeros(shape=(0, n_feature))\n",
        "Y = tf.zeros(shape=(0, 1), dtype=tf.int32)\n",
        "\n",
        "for class_idx in range(N_class):\n",
        "  center = tf.random.uniform(minval=-15, maxval=15, shape=(2, 1))\n",
        "\n",
        "  x1 = center[0] + tf.random.normal(shape=(N, 1))\n",
        "  x2 = center[1] + tf.random.normal(shape=(N, 1))\n",
        "\n",
        "  x = tf.concat((x1, x2), axis=1)\n",
        "  y = class_idx*tf.ones(shape=(N, 1), dtype=tf.int32)\n",
        "\n",
        "  X = tf.concat((X, x), axis=0)\n",
        "  Y = tf.concat((Y, y), axis=0)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "\n",
        "  model = Dense(units=n_class, activation='softmax')\n",
        "  loss_object = SparseCategoricalCrossentropy()\n",
        "\n",
        "  for x, y, in dataset:\n",
        "    predictions = model(x)\n",
        "    loss = loss_object(y, predictions)\n",
        "    print(loss.numpy())"
      ],
      "metadata": {
        "id": "aHvOZVVw2D1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-5: Categorical Cross Entropy"
      ],
      "metadata": {
        "id": "C6d3v7elq0mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-5-1:CCE Calculation"
      ],
      "metadata": {
        "id": "9p-xOjq3q9oF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "batch_size, n_class = 16, 5\n",
        "\n",
        "predictions = tf.random.uniform(shape=(batch_size, n_class), \n",
        "                                minval = 0, maxval=1,\n",
        "                                dtype=tf.float32)\n",
        "\n",
        "pred_sum = tf.reshape(tf.reduce_sum(predictions, axis=1), (-1, 1)) # raw\n",
        "\n",
        "print(predictions.shape, pred_sum.shape)\n",
        "predictions = predictions/pred_sum\n",
        "\n",
        "labels = tf.random.uniform(shape=(batch_size, ),\n",
        "                          minval=0, maxval=n_class,\n",
        "                           dtype=tf.int32)\n",
        "\n",
        "print(labels)\n",
        "labels = tf.one_hot(labels, n_class)\n",
        "print(labels)\n",
        "\n",
        "loss_object = CategoricalCrossentropy()\n",
        "\n",
        "loss = loss_object(labels, predictions)\n",
        "\n",
        "print(\"CCE(TF): \", loss.numpy())\n",
        "\n",
        "tmp = tf.reduce_mean(tf.reduce_sum(-labels * tf.math.log(predictions), axis=1))\n",
        "print(\"CCE(Man): \", tmp.numpy())"
      ],
      "metadata": {
        "id": "UvoXXurjnNIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code.4-5-2: CCE with Model/datset"
      ],
      "metadata": {
        "id": "nooyTvkNs_2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "N, n_feature = 30, 2\n",
        "batch_size = 16\n",
        "N_class = 5\n",
        "\n",
        "X = tf.zeros(shape=(0, n_feature))\n",
        "Y = tf.zeros(shape=(0,), dtype=tf.int32)\n",
        "\n",
        "for class_idx in range(N_class):\n",
        "  center = tf.random.uniform(minval=-15, maxval=15, shape=(2, 1))\n",
        "\n",
        "  x1 = center[0] + tf.random.normal(shape=(N, 1))\n",
        "  x2 = center[1] + tf.random.normal(shape=(N, 1))\n",
        "\n",
        "  x = tf.concat((x1, x2), axis=1)\n",
        "  y = class_idx*tf.ones(shape=(N,), dtype=tf.int32)\n",
        " \n",
        "  X = tf.concat((X, x), axis=0)\n",
        "  Y = tf.concat((Y, y), axis=0)\n",
        "\n",
        "Y = tf.one_hot(Y, depth=N_class, dtype=tf.int32)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "model = Dense(units=N_class, activation='softmax')\n",
        "loss_object = CategoricalCrossentropy()\n",
        "\n",
        "for x, y in dataset:\n",
        "  predictions = model(x)\n",
        "  loss = loss_object(y, predictions)\n",
        "  print(loss.numpy())\n"
      ],
      "metadata": {
        "id": "alWn6CEcslGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0563f41e-1e1b-4131-a1a3-53a986c2ec1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([], shape=(0, 2), dtype=float32)\n",
            "17.314777\n",
            "15.628297\n",
            "2.43815\n",
            "3.915153\n",
            "10.380538\n",
            "7.5922623\n",
            "2.1312292\n",
            "9.780682\n",
            "17.311567\n",
            "16.755621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OvHTOrwQtIZC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}